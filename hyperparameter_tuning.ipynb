{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "import os\n",
    "import shutil\n",
    "import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "experiment_name = 'HyperDrive-Experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verifying that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "key = \"Heart Failure Prediction\"\n",
    "description_text = \"Heart Failure Prediction DataSet for Udacity Project 3\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "#if not found:\n",
    "#        # Creating Dataset and register it into Workspace\n",
    "#        example_data = 'https://github.com/EugeniaSilantjeva/nd00333-capstone/blob/master/heart_failure_clinical_records_dataset.csv'\n",
    "#        dataset = Dataset.Tabular.from_delimited_files(example_data)        \n",
    "#        #Registering Dataset in Workspace\n",
    "#        dataset = dataset.register(workspace=ws,\n",
    "#                                   name=key,\n",
    "#                                   description=description_text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n",
    "\n",
    "I chose Logisit Regression because it is a widely used classifier suitable for a binary classification problem.\n",
    "\n",
    "The model will be trained using different combinations of C and max_iter hyperparameters. C is inverse of regularization strength. Like in support vector machines, smaller values specify stronger regularization. max_iter is the maximum number of iterations taken for the solvers to converge.  \n",
    "\n",
    "I want to find the best C and max_iter combination to have a correct hyperparameter model that is trained with the right number of iterations it requires to converge and does not suffer from overfitting to show a good generalisation performance on the test data. \n",
    "\n",
    "I chose BanditPolicy as the early termination policy. According Microsoft documentation, Bandit policy is \"based on slack criteria, and a frequency and delay interval for evaluation.\" Early termination is required for RandomParameterSampling. It does not allow us to try multiple combinations of the hyperparameters of the search space by reducing the amount of iterations/runs (a combination of the hyperparameters is tried during each iteration) that will not give better results than the ones that have already been completed. This helps save a lot of time and resources. Reference: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py\n",
    "\n",
    "In configuration settings, besides the estimator and BanditPolicy, I specified weighted under the curve  **AUC_weighted** as the primary metric to optimise because it handles unbalanced data. I defined the sampling method over the search space: RandomParameterSampling. This method chooses randomply the best hyperparameters to optimise for speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# An early termination policy. \n",
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "\n",
    "# Different parameters that will be used during training\n",
    "param_sampling = RandomParameterSampling( \n",
    "    {\n",
    "    '--C': choice(0.0001, 0.001, 0.01, 0.1, 1,10,100,1000),\n",
    "    '--max_iter': choice(100, 200, 300, 400, 500)\n",
    "    }\n",
    ")\n",
    "\n",
    "if \"training\" not in os.listdir():\n",
    "    os.mkdir(\"./training\")\n",
    "\n",
    "script_folder = \"./training\"    \n",
    "    \n",
    "# Reference: lesson 6.3: copying the training file into the script folder\n",
    "shutil.copy('./train.py', script_folder)\n",
    "\n",
    "\n",
    "# Estimator and hyperdrive config\n",
    "estimator = SKLearn(source_directory = script_folder, \n",
    "              compute_target=cpu_cluster, \n",
    "              entry_script=\"train.py\")\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator, \n",
    "                          hyperparameter_sampling=param_sampling,\n",
    "                          policy=early_termination_policy, \n",
    "                          primary_metric_name = \"AUC_weighted\", \n",
    "                          primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "                          max_total_runs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(config = hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#reference: lesson 6.3: running a history widget to show the progress\n",
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output = True)\n",
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Getting the best run and displaying its properties\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "\n",
    "print(parameter_values)\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('AUC_weighted:', best_run_metrics['AUC_weighted'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Saving the best model\n",
    "best_run.download_file(\"/outputs/model.joblib\", \"Logistic_Reg_Heart_Failure.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
